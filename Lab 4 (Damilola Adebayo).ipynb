{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "310ae23a-9a89-4a7b-a729-758a9fcd0f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 00:13:51.809310: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-08 00:13:51.835058: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-08 00:13:51.835088: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-08 00:13:51.835990: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-08 00:13:51.840121: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-08 00:13:52.255760: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e038bcb8-e55b-4a19-96eb-c51baf0c57db",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 100\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7ca550-7072-4201-9571-010bd55d3b38",
   "metadata": {},
   "source": [
    "## Defining the BasicLSTM, GRU and MGU Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57570a99-ffd9-4a4c-80d5-0274e38de934",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# RNN Cell implementations\n",
    "class BasicLSTM_cell(object):\n",
    "    def __init__(self, input_units, hidden_units, output_units):\n",
    "        # Initialize weights and biases\n",
    "        self.input_units = input_units\n",
    "        self.hidden_units = hidden_units\n",
    "        self.output_units = output_units\n",
    "        \n",
    "        # Input gate weights\n",
    "        self.Wi = tf.Variable(tf.zeros([self.input_units, self.hidden_units]))\n",
    "        self.Ui = tf.Variable(tf.zeros([self.hidden_units, self.hidden_units]))\n",
    "        self.bi = tf.Variable(tf.zeros([self.hidden_units]))\n",
    "        \n",
    "        # Forget gate weights\n",
    "        self.Wf = tf.Variable(tf.zeros([self.input_units, self.hidden_units]))\n",
    "        self.Uf = tf.Variable(tf.zeros([self.hidden_units, self.hidden_units]))\n",
    "        self.bf = tf.Variable(tf.zeros([self.hidden_units]))\n",
    "        \n",
    "        # Output gate weights\n",
    "        self.Woutg = tf.Variable(tf.zeros([self.input_units, self.hidden_units]))\n",
    "        self.Uoutg = tf.Variable(tf.zeros([self.hidden_units, self.hidden_units]))\n",
    "        self.boutg = tf.Variable(tf.zeros([self.hidden_units]))\n",
    "        \n",
    "        # Cell state weights\n",
    "        self.Wc = tf.Variable(tf.zeros([self.input_units, self.hidden_units]))\n",
    "        self.Uc = tf.Variable(tf.zeros([self.hidden_units, self.hidden_units]))\n",
    "        self.bc = tf.Variable(tf.zeros([self.hidden_units]))\n",
    "        \n",
    "        # Output layer weights\n",
    "        self.Wo = tf.Variable(tf.random.truncated_normal([self.hidden_units, self.output_units], mean=0, stddev=.02))\n",
    "        self.bo = tf.Variable(tf.random.truncated_normal([self.output_units], mean=0, stddev=.02))\n",
    "    \n",
    "    def initialize_state(self, batch_size):\n",
    "        # Initialize hidden state and cell state with zeros\n",
    "        h_init = tf.zeros([batch_size, self.hidden_units])\n",
    "        c_init = tf.zeros([batch_size, self.hidden_units])\n",
    "        return tf.stack([h_init, c_init])\n",
    "    \n",
    "    def Lstm(self, previous_hidden_memory, x):\n",
    "        # Unstack previous hidden state and cell state\n",
    "        previous_hidden_state, c_prev = tf.unstack(previous_hidden_memory)\n",
    "        \n",
    "        # Input gate\n",
    "        i = tf.sigmoid(tf.matmul(x, self.Wi) + \n",
    "                      tf.matmul(previous_hidden_state, self.Ui) + self.bi)\n",
    "        \n",
    "        # Forget gate\n",
    "        f = tf.sigmoid(tf.matmul(x, self.Wf) + \n",
    "                      tf.matmul(previous_hidden_state, self.Uf) + self.bf)\n",
    "        \n",
    "        # Output gate\n",
    "        o = tf.sigmoid(tf.matmul(x, self.Woutg) + \n",
    "                      tf.matmul(previous_hidden_state, self.Uoutg) + self.boutg)\n",
    "        \n",
    "        # New cell state candidate\n",
    "        c_ = tf.tanh(tf.matmul(x, self.Wc) + \n",
    "                    tf.matmul(previous_hidden_state, self.Uc) + self.bc)\n",
    "        \n",
    "        # Update cell state\n",
    "        c = f * c_prev + i * c_\n",
    "        \n",
    "        # Update hidden state\n",
    "        current_hidden_state = o * tf.tanh(c)\n",
    "        \n",
    "        return tf.stack([current_hidden_state, c])\n",
    "    \n",
    "    def process_sequence(self, inputs, initial_state=None):\n",
    "        # Process a sequence of inputs\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        \n",
    "        # Initialize state if not provided\n",
    "        if initial_state is None:\n",
    "            initial_state = self.initialize_state(batch_size)\n",
    "        \n",
    "        # Transpose inputs to time-major format\n",
    "        inputs_time_major = tf.transpose(inputs, perm=[1, 0, 2])\n",
    "        \n",
    "        # Scan through time steps\n",
    "        all_states = tf.scan(self.Lstm, inputs_time_major, initializer=initial_state)\n",
    "        \n",
    "        # Extract hidden states\n",
    "        all_hidden_states = all_states[:, 0, :, :]\n",
    "        \n",
    "        # Apply output layer to each hidden state\n",
    "        all_outputs = tf.map_fn(lambda h: tf.nn.relu(tf.matmul(h, self.Wo) + self.bo), \n",
    "                               all_hidden_states)\n",
    "        \n",
    "        # Convert back to batch-major format\n",
    "        return tf.transpose(all_outputs, perm=[1, 0, 2])\n",
    "\n",
    "\n",
    "class GRU_cell(object):\n",
    "    def __init__(self, input_units, hidden_units, output_units):\n",
    "        self.input_units = input_units\n",
    "        self.hidden_units = hidden_units\n",
    "        self.output_units = output_units\n",
    "        \n",
    "        # Update gate weights\n",
    "        self.Wz = tf.Variable(tf.zeros([self.input_units, self.hidden_units]))\n",
    "        self.Uz = tf.Variable(tf.zeros([self.hidden_units, self.hidden_units])) \n",
    "        self.bz = tf.Variable(tf.zeros([self.hidden_units]))\n",
    "        \n",
    "        # Reset gate weights\n",
    "        self.Wr = tf.Variable(tf.zeros([self.input_units, self.hidden_units]))\n",
    "        self.Ur = tf.Variable(tf.zeros([self.hidden_units, self.hidden_units]))\n",
    "        self.br = tf.Variable(tf.zeros([self.hidden_units]))\n",
    "        \n",
    "        # Candidate state weights\n",
    "        self.Ws = tf.Variable(tf.zeros([self.input_units, self.hidden_units]))\n",
    "        self.Us = tf.Variable(tf.zeros([self.hidden_units, self.hidden_units]))\n",
    "        self.bs = tf.Variable(tf.zeros([self.hidden_units]))\n",
    "        \n",
    "        # Output layer weights\n",
    "        self.Wo = tf.Variable(tf.random.truncated_normal([self.hidden_units, self.output_units], mean=0, stddev=.02))\n",
    "        self.bo = tf.Variable(tf.random.truncated_normal([self.output_units], mean=0, stddev=.02))\n",
    "    \n",
    "    def initialize_state(self, batch_size):\n",
    "        # Initialize hidden state with zeros\n",
    "        return tf.zeros([batch_size, self.hidden_units])\n",
    "    \n",
    "    def Gru(self, previous_state, x):\n",
    "        # Update gate\n",
    "        z = tf.sigmoid(tf.matmul(x, self.Wz) + \n",
    "                      tf.matmul(previous_state, self.Uz) + self.bz)\n",
    "        \n",
    "        # Reset gate\n",
    "        r = tf.sigmoid(tf.matmul(x, self.Wr) + \n",
    "                      tf.matmul(previous_state, self.Ur) + self.br)\n",
    "        \n",
    "        # Candidate state\n",
    "        s_tilde = tf.tanh(tf.matmul(x, self.Ws) + \n",
    "                         tf.matmul(r * previous_state, self.Us) + self.bs)\n",
    "        \n",
    "        # Update state\n",
    "        current_state = (1 - z) * previous_state + z * s_tilde\n",
    "        \n",
    "        return current_state\n",
    "    \n",
    "    def process_sequence(self, inputs, initial_state=None):\n",
    "        # Process a sequence of inputs\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        \n",
    "        # Initialize state if not provided\n",
    "        if initial_state is None:\n",
    "            initial_state = self.initialize_state(batch_size)\n",
    "        \n",
    "        # Transpose inputs to time-major format\n",
    "        inputs_time_major = tf.transpose(inputs, perm=[1, 0, 2])\n",
    "        \n",
    "        # Scan through time steps\n",
    "        all_states = tf.scan(self.Gru, inputs_time_major, initializer=initial_state)\n",
    "        \n",
    "        # Apply output layer to each hidden state\n",
    "        all_outputs = tf.map_fn(lambda h: tf.nn.relu(tf.matmul(h, self.Wo) + self.bo), \n",
    "                               all_states)\n",
    "        \n",
    "        # Convert back to batch-major format\n",
    "        return tf.transpose(all_outputs, perm=[1, 0, 2])\n",
    "\n",
    "\n",
    "class MGU_cell(object):\n",
    "    def __init__(self, input_units, hidden_units, output_units):\n",
    "        self.input_units = input_units\n",
    "        self.hidden_units = hidden_units\n",
    "        self.output_units = output_units\n",
    "        \n",
    "        # Forget/update gate weights\n",
    "        self.Wf = tf.Variable(tf.zeros([self.input_units, self.hidden_units]))\n",
    "        self.Uf = tf.Variable(tf.zeros([self.hidden_units, self.hidden_units]))\n",
    "        self.bf = tf.Variable(tf.zeros([self.hidden_units]))\n",
    "        \n",
    "        # Candidate state weights\n",
    "        self.Ws = tf.Variable(tf.zeros([self.input_units, self.hidden_units]))\n",
    "        self.Us = tf.Variable(tf.zeros([self.hidden_units, self.hidden_units]))\n",
    "        self.bs = tf.Variable(tf.zeros([self.hidden_units]))\n",
    "        \n",
    "        # Output layer weights\n",
    "        self.Wo = tf.Variable(tf.random.truncated_normal([self.hidden_units, self.output_units], mean=0, stddev=.02))\n",
    "        self.bo = tf.Variable(tf.random.truncated_normal([self.output_units], mean=0, stddev=.02))\n",
    "    \n",
    "    def initialize_state(self, batch_size):\n",
    "        # Initialize hidden state with zeros\n",
    "        return tf.zeros([batch_size, self.hidden_units])\n",
    "    \n",
    "    def Mgu(self, previous_state, x):\n",
    "        # Forget gate (equivalent to update gate)\n",
    "        f = tf.sigmoid(tf.matmul(x, self.Wf) + \n",
    "                     tf.matmul(previous_state, self.Uf) + self.bf)\n",
    "        \n",
    "        # Candidate state\n",
    "        s_tilde = tf.tanh(tf.matmul(x, self.Ws) + \n",
    "                        tf.matmul(f * previous_state, self.Us) + self.bs)\n",
    "        \n",
    "        # Update state\n",
    "        current_state = (1 - f) * previous_state + f * s_tilde\n",
    "        \n",
    "        return current_state\n",
    "    \n",
    "    def process_sequence(self, inputs, initial_state=None):\n",
    "        # Process a sequence of inputs\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        \n",
    "        # Initialize state if not provided\n",
    "        if initial_state is None:\n",
    "            initial_state = self.initialize_state(batch_size)\n",
    "        \n",
    "        # Transpose inputs to time-major format\n",
    "        inputs_time_major = tf.transpose(inputs, perm=[1, 0, 2])\n",
    "        \n",
    "        # Scan through time steps\n",
    "        all_states = tf.scan(self.Mgu, inputs_time_major, initializer=initial_state)\n",
    "        \n",
    "        # Apply output layer to each hidden state\n",
    "        all_outputs = tf.map_fn(lambda h: tf.nn.relu(tf.matmul(h, self.Wo) + self.bo), \n",
    "                               all_states)\n",
    "        \n",
    "        # Convert back to batch-major format\n",
    "        return tf.transpose(all_outputs, perm=[1, 0, 2])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6de0a30-2a7a-4a8b-accc-db3351f7fd7d",
   "metadata": {},
   "source": [
    "## Defining the data loading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9db00c8-6a6b-4f6d-b013-ebb523ee67d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preprocessing functions\n",
    "def load_notmnist_dataset(data_dir):\n",
    "    # List all subdirectories (each represents a class)\n",
    "    class_dirs = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    class_dirs.sort()  # Ensure consistent ordering\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Load images from each class directory\n",
    "    for class_idx, class_dir in enumerate(class_dirs):\n",
    "        class_path = os.path.join(data_dir, class_dir)\n",
    "        file_list = glob.glob(os.path.join(class_path, '*.png')) + glob.glob(os.path.join(class_path, '*.jpg'))\n",
    "        \n",
    "        for file_path in file_list:\n",
    "            try:\n",
    "                # Load image as grayscale\n",
    "                img = Image.open(file_path).convert('L')\n",
    "                img = img.resize((28, 28))  # Resize to consistent dimensions\n",
    "                img_array = np.array(img, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "                \n",
    "                images.append(img_array)\n",
    "                labels.append(class_idx)\n",
    "            except:\n",
    "                # Skip corrupted images\n",
    "                print(f\"Error loading {file_path}, skipping...\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Shuffle the data\n",
    "    indices = np.arange(len(images))\n",
    "    np.random.shuffle(indices)\n",
    "    images = images[indices]\n",
    "    labels = labels[indices]\n",
    "    \n",
    "    # Split into train and test sets (80% train, 20% test)\n",
    "    split_idx = int(len(images) * 0.8)\n",
    "    x_train, x_test = images[:split_idx], images[split_idx:]\n",
    "    y_train, y_test = labels[:split_idx], labels[split_idx:]\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a8636b-9992-46ea-884f-fa224f3c6862",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fc78226-8314-4c0b-b27a-3005c2d1db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_rnn_model(rnn_cell, dataset, epochs=10, batch_size=128, learning_rate=0.001):\n",
    "    # Split dataset into training and testing\n",
    "    (x_train, y_train), (x_test, y_test) = dataset\n",
    "    \n",
    "    # Reshape input for RNN: we'll treat each row of the image as a time step\n",
    "    # So for a 28x28 image, we have 28 time steps of 28 features each\n",
    "    x_train = x_train.reshape(-1, 28, 28)\n",
    "    x_test = x_test.reshape(-1, 28, 28)\n",
    "    \n",
    "    # Convert to TensorFlow datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=10000).batch(batch_size)\n",
    "    \n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    test_dataset = test_dataset.batch(batch_size)\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    # Define loss function\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_accuracy': [],\n",
    "        'test_loss': [],\n",
    "        'test_accuracy': [],\n",
    "        'time':[]\n",
    "    }\n",
    "    \n",
    "    # Get all trainable variables\n",
    "    def get_trainable_vars(cell):\n",
    "        return [var for var in cell.__dict__.values() if isinstance(var, tf.Variable)]\n",
    "        \n",
    "    start_time =time.time()\n",
    "    end_time = 0\n",
    "    \n",
    "    trainable_vars = get_trainable_vars(rnn_cell)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Training metrics for this epoch\n",
    "        train_loss_total = 0.0\n",
    "        train_accuracy_total = 0.0\n",
    "        train_batch_count = 0\n",
    "        \n",
    "        # Test metrics for this epoch\n",
    "        test_loss_total = 0.0\n",
    "        test_accuracy_total = 0.0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        # Training\n",
    "        for x_batch, y_batch in train_dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Forward pass\n",
    "                predictions = rnn_cell.process_sequence(x_batch)\n",
    "                # Use the last output for classification\n",
    "                logits = predictions[:, -1, :]\n",
    "                loss = loss_fn(y_batch, logits)\n",
    "            \n",
    "            # Calculate gradients\n",
    "            gradients = tape.gradient(loss, trainable_vars)\n",
    "            \n",
    "            # Apply gradients\n",
    "            optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "            \n",
    "            # Update metrics\n",
    "            train_loss_total += loss.numpy()\n",
    "            train_accuracy_total += tf.reduce_mean(\n",
    "                tf.cast(tf.equal(tf.argmax(logits, axis=1), y_batch), tf.float32)\n",
    "            ).numpy()\n",
    "            train_batch_count += 1\n",
    "        \n",
    "        # Calculate average training metrics\n",
    "        avg_train_loss = train_loss_total / train_batch_count\n",
    "        avg_train_accuracy = train_accuracy_total / train_batch_count\n",
    "\n",
    "        \n",
    "        end_time = time.time()\n",
    "\n",
    "        \n",
    "        # Testing\n",
    "        for x_batch, y_batch in test_dataset:\n",
    "            # Forward pass\n",
    "            predictions = rnn_cell.process_sequence(x_batch)\n",
    "            # Use the last output for classification\n",
    "            logits = predictions[:, -1, :]\n",
    "            loss = loss_fn(y_batch, logits)\n",
    "            \n",
    "            # Update metrics\n",
    "            test_loss_total += loss.numpy()\n",
    "            test_accuracy_total += tf.reduce_mean(\n",
    "                tf.cast(tf.equal(tf.argmax(logits, axis=1), y_batch), tf.float32)\n",
    "            ).numpy()\n",
    "            test_batch_count += 1\n",
    "        \n",
    "        # Calculate average test metrics\n",
    "        avg_test_loss = test_loss_total / test_batch_count\n",
    "        avg_test_accuracy = test_accuracy_total / test_batch_count\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_accuracy'].append(avg_train_accuracy)\n",
    "        history['test_loss'].append(avg_test_loss)\n",
    "        history['test_accuracy'].append(avg_test_accuracy)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, '\n",
    "              f'Loss: {avg_train_loss:.4f}, '\n",
    "              f'Accuracy: {avg_train_accuracy:.4f}, '\n",
    "              f'Test Loss: {avg_test_loss:.4f}, '\n",
    "              f'Test Accuracy: {avg_test_accuracy:.4f}')\n",
    "    history['time'].append(end_time-start_time)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6577c6-0894-4ae6-88aa-49a2d23bc01e",
   "metadata": {},
   "source": [
    "## Running all the trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdacf133-7d15-447e-8aa3-52978cc1a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments\n",
    "def run_experiments(data_dir, num_trials=3, hidden_units=64, epochs=5):\n",
    "    results = {\n",
    "        'rnn': [],\n",
    "        'gru': [],\n",
    "        'mgu': []\n",
    "    }\n",
    "    \n",
    "    input_units = 28  \n",
    "    output_units = 10  # since notMNIST has 10 classes (A to J)\n",
    "    \n",
    "    # Summary table for classification errors\n",
    "    error_summary = {\n",
    "        'rnn_train_error': [],\n",
    "        'rnn_test_error': [],\n",
    "        'rnn_time': [],\n",
    "        'gru_train_error': [],\n",
    "        'gru_test_error': [],\n",
    "        'gru_time':[],\n",
    "        'mgu_train_error': [],\n",
    "        'mgu_test_error': [],\n",
    "        'mgu_time':[]\n",
    "    }\n",
    "    \n",
    "    for trial in range(num_trials):\n",
    "        print(f\"Trial {trial + 1}/{num_trials}\")\n",
    "        \n",
    "        # Load datase\n",
    "        dataset = load_notmnist_dataset(data_dir)\n",
    "        \n",
    "        # Train Basic RNN\n",
    "        print(\"Training Basic RNN...\")\n",
    "        rnn_cell = BasicLSTM_cell(input_units, hidden_units, output_units)\n",
    "        rnn_history = train_rnn_model(rnn_cell, dataset, epochs=epochs)\n",
    "        results['rnn'].append(rnn_history)\n",
    "        \n",
    "        # Calculating final errors for RNN\n",
    "        final_rnn_train_error = 1.0 - rnn_history['train_accuracy'][-1]\n",
    "        final_rnn_test_error = 1.0 - rnn_history['test_accuracy'][-1]\n",
    "        final_rnn_time = rnn_history['time'][-1]\n",
    "        error_summary['rnn_train_error'].append(final_rnn_train_error)\n",
    "        error_summary['rnn_test_error'].append(final_rnn_test_error)\n",
    "        error_summary['rnn_time'].append(final_rnn_time)\n",
    "        \n",
    "        # Train GRU\n",
    "        print(\"Training GRU...\")\n",
    "        gru_cell = GRU_cell(input_units, hidden_units, output_units)\n",
    "        gru_history = train_rnn_model(gru_cell, dataset, epochs=epochs)\n",
    "        results['gru'].append(gru_history)\n",
    "        \n",
    "        # Calculate final errors for GRU\n",
    "        final_gru_train_error = 1.0 - gru_history['train_accuracy'][-1]\n",
    "        final_gru_test_error = 1.0 - gru_history['test_accuracy'][-1]\n",
    "        final_gru_time = gru_history['time'][-1]\n",
    "        error_summary['gru_train_error'].append(final_gru_train_error)\n",
    "        error_summary['gru_test_error'].append(final_gru_test_error)\n",
    "        error_summary['gru_time'].append(final_gru_time)\n",
    "        \n",
    "        # Train MGU\n",
    "        print(\"Training MGU...\")\n",
    "        mgu_cell = MGU_cell(input_units, hidden_units, output_units)\n",
    "        mgu_history = train_rnn_model(mgu_cell, dataset, epochs=epochs)\n",
    "        results['mgu'].append(mgu_history)\n",
    "        \n",
    "        # Calculating final errors for MGU\n",
    "        final_mgu_train_error = 1.0 - mgu_history['train_accuracy'][-1]\n",
    "        final_mgu_test_error = 1.0 - mgu_history['test_accuracy'][-1]\n",
    "        final_mgu_time = mgu_history['time'][-1]\n",
    "        error_summary['mgu_train_error'].append(final_mgu_train_error)\n",
    "        error_summary['mgu_test_error'].append(final_mgu_test_error)\n",
    "        error_summary['mgu_time'].append(final_mgu_time)\n",
    "        \n",
    "        # Reporting errors for this trial\n",
    "        print(f\"\\nClassification Error Report for Trial {trial + 1}:\")\n",
    "        print(f\"RNN - Training Error: {final_rnn_train_error:.4f}, Test Error: {final_rnn_test_error:.4f}, time: {final_rnn_time:.4f} seconds\")\n",
    "        print(f\"GRU - Training Error: {final_gru_train_error:.4f}, Test Error: {final_gru_test_error:.4f}, time: {final_gru_time:.4f} seconds\")\n",
    "        print(f\"MGU - Training Error: {final_mgu_train_error:.4f}, Test Error: {final_mgu_test_error:.4f}, time: {final_mgu_time:.4f} seconds\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Calculate and report average errors across all trials\n",
    "    print(\"\\nSummary of Classification Errors Across All Trials:\")\n",
    "    print(f\"RNN - Avg Training Error: {np.mean(error_summary['rnn_train_error']):.4f} ± {np.std(error_summary['rnn_train_error']):.4f}\")\n",
    "    print(f\"RNN - Avg Test Error: {np.mean(error_summary['rnn_test_error']):.4f} ± {np.std(error_summary['rnn_test_error']):.4f}\")\n",
    "    print(f\"RNN - Avg Time: {np.mean(error_summary['rnn_time']):.4f} ± {np.std(error_summary['rnn_time']):.4f} seconds\\n\")\n",
    "    \n",
    "    print(f\"GRU - Avg Training Error: {np.mean(error_summary['gru_train_error']):.4f} ± {np.std(error_summary['gru_train_error']):.4f}\")\n",
    "    print(f\"GRU - Avg Test Error: {np.mean(error_summary['gru_test_error']):.4f} ± {np.std(error_summary['gru_test_error']):.4f}\")\n",
    "    print(f\"GRU - Avg Time: {np.mean(error_summary['gru_time']):.4f} ± {np.std(error_summary['gru_time']):.4f} seconds\\n\")\n",
    "    \n",
    "    print(f\"MGU - Avg Training Error: {np.mean(error_summary['mgu_train_error']):.4f} ± {np.std(error_summary['mgu_train_error']):.4f}\")\n",
    "    print(f\"MGU - Avg Test Error: {np.mean(error_summary['mgu_test_error']):.4f} ± {np.std(error_summary['mgu_test_error']):.4f}\")\n",
    "    print(f\"MGU - Avg Time: {np.mean(error_summary['mgu_time']):.4f} ± {np.std(error_summary['mgu_time']):.4f} seconds\")\n",
    "    \n",
    "    return results, error_summary\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d5fe98-73ac-43da-b621-b86320020950",
   "metadata": {},
   "source": [
    "## Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d038e1f-d470-4a40-9c20-504f56ba3268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "def plot_results(results):\n",
    "    models = ['rnn', 'gru', 'mgu']\n",
    "    metrics = ['train_accuracy', 'test_accuracy']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        for model in models:\n",
    "            # Calculate mean and std across trials\n",
    "            trials_data = [trial[metric] for trial in results[model]]\n",
    "            mean_data = np.mean(trials_data, axis=0)\n",
    "            std_data = np.std(trials_data, axis=0)\n",
    "            \n",
    "            # Plot mean with std as shaded area\n",
    "            epochs = range(1, len(mean_data) + 1)\n",
    "            ax.plot(epochs, mean_data, label=model.upper())\n",
    "            ax.fill_between(epochs, mean_data - std_data, mean_data + std_data, alpha=0.2)\n",
    "        \n",
    "        ax.set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "        ax.set_xlabel('Epochs')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('rnn_comparison.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2a8fe4-e24d-4b96-982d-f52d52efaef2",
   "metadata": {},
   "source": [
    "## Running the experiment across the three trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90837e80-b5e9-4fab-aa88-4af551c26598",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \n",
    "    data_dir = r\"./notMNIST_small\"\n",
    "    \n",
    "    # Run experiments\n",
    "    results, error_summary = run_experiments(data_dir, num_trials=3, hidden_units=64, epochs=5)\n",
    "    \n",
    "    # Plot results\n",
    "    plot_results(results)\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Data for the bar chart\n",
    "    models = ['RNN', 'GRU', 'MGU']\n",
    "    train_errors = [\n",
    "        np.mean(error_summary['rnn_train_error']), \n",
    "        np.mean(error_summary['gru_train_error']), \n",
    "        np.mean(error_summary['mgu_train_error'])\n",
    "    ]\n",
    "    test_errors = [\n",
    "        np.mean(error_summary['rnn_test_error']), \n",
    "        np.mean(error_summary['gru_test_error']), \n",
    "        np.mean(error_summary['mgu_test_error'])\n",
    "    ]\n",
    "    train_std = [\n",
    "        np.std(error_summary['rnn_train_error']), \n",
    "        np.std(error_summary['gru_train_error']), \n",
    "        np.std(error_summary['mgu_train_error'])\n",
    "    ]\n",
    "    test_std = [\n",
    "        np.std(error_summary['rnn_test_error']), \n",
    "        np.std(error_summary['gru_test_error']), \n",
    "        np.std(error_summary['mgu_test_error'])\n",
    "    ]\n",
    "    train_n_test_times = [\n",
    "        np.std(error_summary['rnn_time']), \n",
    "        np.std(error_summary['gru_time']), \n",
    "        np.std(error_summary['mgu_time'])\n",
    "    ]\n",
    "    \n",
    "    # Positioning for bars\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Create bars\n",
    "    plt.bar(x - width/2, train_errors, width, label='Training Error', yerr=train_std, capsize=5)\n",
    "    plt.bar(x + width/2, test_errors, width, label='Test Error', yerr=test_std, capsize=5)\n",
    "    \n",
    "    \n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Classification Error')\n",
    "    plt.title('Classification Error Comparison: RNN vs GRU vs MGU')\n",
    "    plt.xticks(x, models)\n",
    "    plt.legend()\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "   \n",
    "    # Adding value labels on top of the bars\n",
    "    for i, v in enumerate(train_errors):\n",
    "        plt.text(i - width/2, v + 0.01, f'{v:.4f}', ha='center')\n",
    "    for i, v in enumerate(test_errors):\n",
    "        plt.text(i + width/2, v + 0.01, f'{v:.4f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('classification_error_comparison.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Creating the bar chart for the average time it took each model across the three trials\n",
    "    plt.bar(x, train_n_test_times) \n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Classification Time')\n",
    "    plt.title('Classification Time Comparison: RNN vs GRU vs MGU')\n",
    "    plt.xticks(x, models)\n",
    "    plt.legend()\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6960659-5631-40d8-a40e-3eb203314aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1/3\n",
      "Error loading ./notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png, skipping...\n",
      "Error loading ./notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png, skipping...\n",
      "Training Basic RNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 00:15:46.690742: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-08 00:15:46.707027: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-08 00:15:46.707064: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-08 00:15:46.711065: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-08 00:15:46.711096: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-08 00:15:46.931345: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-08 00:15:46.931389: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-08 00:15:46.931394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-05-08 00:15:46.931418: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-08 00:15:46.931434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 17768 MB memory:  -> device: 0, name: NVIDIA RTX A4500, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2025-05-08 00:15:47.564655: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-05-08 00:15:48.346283: I external/local_xla/xla/service/service.cc:168] XLA service 0x429c9a60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-05-08 00:15:48.346330: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A4500, Compute Capability 8.6\n",
      "2025-05-08 00:15:48.349583: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-05-08 00:15:48.361151: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746688548.408948   41673 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f6731975ab0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f6731975ab0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/5, Loss: 2.0269, Accuracy: 0.2822, Test Loss: 1.6832, Test Accuracy: 0.4341\n",
      "Epoch 2/5, Loss: 1.3074, Accuracy: 0.6118, Test Loss: 0.9238, Test Accuracy: 0.7326\n",
      "Epoch 3/5, Loss: 0.6880, Accuracy: 0.8062, Test Loss: 0.5678, Test Accuracy: 0.8364\n",
      "Epoch 4/5, Loss: 0.5021, Accuracy: 0.8623, Test Loss: 0.4723, Test Accuracy: 0.8692\n",
      "Epoch 5/5, Loss: 0.4220, Accuracy: 0.8803, Test Loss: 0.4497, Test Accuracy: 0.8726\n",
      "Training GRU...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
